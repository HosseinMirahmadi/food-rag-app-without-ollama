import streamlit as st
import pandas as pd
import os
import shutil

from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace  # Ø¬Ø¯ÛŒØ¯: ChatHuggingFace Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
from langchain_community.vectorstores import Chroma

st.set_page_config(page_title="ØºØ°Ø§ Ùˆ Ø±Ø³ØªÙˆØ±Ø§Ù†", page_icon="ğŸ¥—", layout="wide")

st.markdown("""
<style>
    @import url('https://v1.fontapi.ir/css/Vazir');
    html, body, [class*="css"] { font-family: 'Vazir', 'Tahoma', sans-serif; direction: rtl; text-align: right; }
    .stApp { background: linear-gradient(135deg, #0f2027, #203a43, #2c5364); }
    .stTextInput > div > div > input, .stTextArea > div > div > textarea { direction: rtl; text-align: right; }
    .card { background-color: #1e1e1e; padding: 20px; border-radius: 16px; box-shadow: 0 8px 20px rgba(0,0,0,0.4); margin-bottom: 20px; border: 1px solid #333; }
    .title { font-size: 2.4em; font-weight: 800; color: #6ee7b7; text-align: right; }
    .subtitle { color: #a7f3d0; font-size: 1.1em; text-align: right; margin-top: 5px; }
    .result-text { color: #e2e8f0; font-size: 1.1em; line-height: 1.8; text-align: right; direction: rtl; }
    [data-testid="stDataFrame"] { direction: rtl; text-align: right; }
    .stDataFrame div[role="columnheader"], .stDataFrame div[role="gridcell"] { text-align: right !important; justify-content: right !important; }
    .stAlert { direction: rtl; text-align: right; }
</style>
""", unsafe_allow_html=True)

PERSIST_DIRECTORY = "./chroma_db_food"
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

@st.cache_resource
def load_embedding_model():
    return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

def create_knowledge_base(urls):
    if os.path.exists(PERSIST_DIRECTORY):
        try:
            shutil.rmtree(PERSIST_DIRECTORY)
        except:
            pass
    try:
        loader = WebBaseLoader(urls)
        data = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        all_splits = text_splitter.split_documents(data)
        embedding_model = load_embedding_model()
        vector_db = Chroma.from_documents(
            documents=all_splits,
            embedding=embedding_model,
            persist_directory=PERSIST_DIRECTORY
        )
        return True, len(all_splits)
    except Exception as e:
        return False, str(e)

def perform_rag_search(query):
    embedding_model = load_embedding_model()
    vector_db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding_model)
    retriever = vector_db.as_retriever(search_kwargs={"k": 5})
    docs = retriever.invoke(query)
    
    context_text = "\n\n".join([doc.page_content for doc in docs])
    
    # --- Ø¬Ø¯ÛŒØ¯: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ChatHuggingFace Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ conversational ---
    base_llm = HuggingFaceEndpoint(
        repo_id="HuggingFaceH4/zephyr-7b-beta",
        huggingfacehub_api_token=st.secrets["HUGGINGFACEHUB_API_TOKEN"],
        temperature=0.7,
        max_new_tokens=512,
        repetition_penalty=1.1
    )
    
    llm = ChatHuggingFace(llm=base_llm)  # Ø§ÛŒÙ† Ø®Ø· Ø§Ø±ÙˆØ± task Ø±Ùˆ Ú©Ø§Ù…Ù„ Ø­Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
    
    messages = [
        {"role": "system", "content": "ØªÙˆ ÛŒÚ© Ù…ØªØ®ØµØµ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ØºØ°Ø§ Ùˆ Ø¢Ø´Ù¾Ø²ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ù‡Ø³ØªÛŒ. ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡. Ø§Ø² Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ÛŒØ§ Ù‡Ø± Ø²Ø¨Ø§Ù† Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†."},
        {"role": "user", "content": f"Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø· Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹:\n{context_text}\n\nØ³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {query}\n\nÙ¾Ø§Ø³Ø® Ú©Ø§Ù…Ù„ØŒ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…ÙÛŒØ¯ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø¯Ù‡:"}
    ]
    
    response = llm.invoke(messages).content  # .content Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† ÙÙ‚Ø· Ù…ØªÙ† Ù¾Ø§Ø³Ø®
    return response, docs

# Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ (Ù‡Ù…ÙˆÙ† Ù‚Ø¨Ù„ÛŒ)
st.markdown("""
<div class="card">
    <div class="title">ğŸ¥— Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ØºØ°Ø§ Ùˆ Ø±Ø³ØªÙˆØ±Ø§Ù†</div>
    <div class="subtitle">Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ù†ÙˆÛŒ Ø±Ø³ØªÙˆØ±Ø§Ù†â€ŒÙ‡Ø§ØŒ Ø¯Ø³ØªÙˆØ± Ù¾Ø®Øªâ€ŒÙ‡Ø§ Ùˆ Ù…Ù‚Ø§Ù„Ø§Øª ØºØ°Ø§ÛŒÛŒ</div>
</div>
""", unsafe_allow_html=True)

st.markdown("### ğŸ”— Ù…Ø±Ø­Ù„Ù‡ Û±: Ù…Ù†Ø§Ø¨Ø¹ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ")
with st.container():
    input_urls = st.text_area(
        "Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ù‡Ø± Ø®Ø· ÛŒÚ© Ù„ÛŒÙ†Ú©):",
        height=100,
        placeholder="https://example.com/menu",
        value="https://fa.wikipedia.org/wiki/Ø¢Ø´Ù¾Ø²ÛŒ_Ø§ÛŒØ±Ø§Ù†ÛŒ\nhttps://fa.wikipedia.org/wiki/Ú©Ø¨Ø§Ø¨"
    )

st.markdown("### ğŸ‘¨â€ğŸ³ Ù…Ø±Ø­Ù„Ù‡ Û²: Ù¾Ø±Ø¯Ø§Ø²Ø´")
if st.button("ğŸ³ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ"):
    if input_urls.strip():
        url_list = [u.strip() for u in input_urls.split('\n') if u.strip()]
        with st.spinner('Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ø³Ø§Ø®Øª Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´...'):
            success, result = create_knowledge_base(url_list)
        if success:
            st.success(f"âœ… Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯! {result} Ø¨Ø®Ø´ Ù…ØªÙ†ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
            st.session_state["db_ready"] = True
        else:
            st.error(f"âŒ Ø®Ø·Ø§: {result}")
    else:
        st.warning("âš ï¸ Ù„Ø·ÙØ§Ù‹ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ù„ÛŒÙ†Ú© ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")

if st.session_state.get("db_ready"):
    st.markdown("### ğŸ½ï¸ Ù…Ø±Ø­Ù„Ù‡ Û³: Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø®")
    col1, col2 = st.columns([4, 1])
    with col1:
        query = st.text_input("Ø³ÙˆØ§Ù„ Ø´Ù…Ø§:", placeholder="Ù…Ø«Ù„Ø§Ù‹: Ú©Ø¨Ø§Ø¨ Ú©ÙˆØ¨ÛŒØ¯Ù‡ Ø®ÙˆØ¨ Ú†Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯ØŸ")
    with col2:
        st.markdown("<div style='margin-top: 28px;'></div>", unsafe_allow_html=True)
        search = st.button("ğŸ” Ø¬Ø³ØªØ¬Ùˆ", use_container_width=True)

    if search and query:
        with st.spinner('Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø³ØªØ¬Ùˆ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®...'):
            try:
                ai_response, source_docs = perform_rag_search(query)
                
                st.markdown(f"""
                <div class="card">
                    <h3 style="color:#fbbf24; text-align:right; margin-bottom:10px;">ğŸ• Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ:</h3>
                    <div class="result-text">{ai_response}</div>
                </div>
                """, unsafe_allow_html=True)

                st.markdown("### ğŸ“œ Ù…Ù†Ø§Ø¨Ø¹ ÛŒØ§ÙØª Ø´Ø¯Ù‡")
                table_data = []
                for idx, doc in enumerate(source_docs):
                    table_data.append({
                        "Ø±ØªØ¨Ù‡": idx + 1,
                        "Ù…ØªÙ† (Ø®Ù„Ø§ØµÙ‡)": doc.page_content[:150] + "...",
                        "Ù„ÛŒÙ†Ú© Ù…Ù†Ø¨Ø¹": doc.metadata.get('source', 'Ù†Ø§Ù…Ø´Ø®Øµ'),
                    })
                df = pd.DataFrame(table_data)
                st.dataframe(
                    df,
                    use_container_width=True,
                    column_config={
                        "Ù„ÛŒÙ†Ú© Ù…Ù†Ø¨Ø¹": st.column_config.LinkColumn("Ù„ÛŒÙ†Ú© Ú©Ø§Ù…Ù„"),
                        "Ø±ØªØ¨Ù‡": st.column_config.NumberColumn("Ø±ØªØ¨Ù‡", format="%d")
                    },
                    hide_index=True
                )
            except Exception as e:
                st.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®: {e}")