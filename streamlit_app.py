import streamlit as st
import pandas as pd
import os
import shutil
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace
from langchain_community.vectorstores import Chroma

st.set_page_config(page_title="ØºØ°Ø§ Ùˆ Ø±Ø³ØªÙˆØ±Ø§Ù†", page_icon="ğŸ¥—", layout="centered")

st.markdown("""
<style>
    @import url('https://v1.fontapi.ir/css/Vazir');
    html, body, [class*="css"] { font-family: 'Vazir', 'Tahoma', sans-serif; direction: rtl; text-align: right; }
    .stApp { background: linear-gradient(135deg, #0f2027, #203a43, #2c5364); }
    .stTextInput > div > div > input { direction: ltr; text-align: left; } /* Ú†Ù¾â€ŒÚ†ÛŒÙ† Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù† */
    .card { background-color: #1e1e1e; padding: 15px; border-radius: 12px; margin-bottom: 15px; border: 1px solid #333; }
    .title { font-size: 2em; color: #6ee7b7; text-align: right; }
    .result-text { color: #e2e8f0; font-size: 1.1em; line-height: 1.8; text-align: right; direction: rtl; }
</style>
""", unsafe_allow_html=True)

PERSIST_DIRECTORY = "./chroma_db_food_mobile"

# --- Ø¨Ø®Ø´ Ø¯Ø±ÛŒØ§ÙØª ØªÙˆÚ©Ù† Ø§Ø² Ú©Ø§Ø±Ø¨Ø± (Ø§Ù…Ù†) ---
st.markdown('<div class="card"><div class="title">ğŸ¥— Ø¢Ø´Ù¾Ø²ÛŒØ§Ø± Ù‡Ù…Ø±Ø§Ù‡</div></div>', unsafe_allow_html=True)

# Ú©Ø§Ø¯Ø± Ø±Ù…Ø² Ø¹Ø¨ÙˆØ± Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† ØªÙˆÚ©Ù†
hf_token = st.text_input("ğŸ”‘ ØªÙˆÚ©Ù† HuggingFace Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:", type="password", placeholder="hf_...")

if not hf_token:
    st.warning("âš ï¸ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ØŒ Ù„Ø·ÙØ§Ù‹ ØªÙˆÚ©Ù† Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
    st.stop() # ØªØ§ ÙˆÙ‚ØªÛŒ ØªÙˆÚ©Ù† Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ù‚ÛŒÙ‡ Ú©Ø¯ Ø§Ø¬Ø±Ø§ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯

# --- ØªÙˆØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ---

@st.cache_resource
def load_embedding_model(token):
    return HuggingFaceInferenceAPIEmbeddings(
        api_key=token,
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )

def create_knowledge_base(urls, token):
    if os.path.exists(PERSIST_DIRECTORY):
        try: shutil.rmtree(PERSIST_DIRECTORY)
        except: pass
    try:
        loader = WebBaseLoader(urls)
        data = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        all_splits = text_splitter.split_documents(data)
        
        embedding_model = load_embedding_model(token)
        vector_db = Chroma.from_documents(documents=all_splits, embedding=embedding_model, persist_directory=PERSIST_DIRECTORY)
        return True, len(all_splits)
    except Exception as e:
        return False, str(e)

def perform_rag_search(query, token):
    embedding_model = load_embedding_model(token)
    vector_db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding_model)
    retriever = vector_db.as_retriever(search_kwargs={"k": 3})
    docs = retriever.invoke(query)
    context_text = "\n\n".join([doc.page_content for doc in docs])
    
    base_llm = HuggingFaceEndpoint(
        repo_id="HuggingFaceH4/zephyr-7b-beta",
        huggingfacehub_api_token=token,
        max_new_tokens=512,
        temperature=0.7
    )
    
    llm = ChatHuggingFace(llm=base_llm)
    messages = [
        {"role": "system", "content": "ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± Ø¢Ø´Ù¾Ø²ÛŒ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒ. Ú©ÙˆØªØ§Ù‡ Ùˆ Ø®Ù„Ø§ØµÙ‡ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡."},
        {"role": "user", "content": f"Ù…ØªÙ†:\n{context_text}\n\nØ³ÙˆØ§Ù„: {query}\n\nÙ¾Ø§Ø³Ø®:"}
    ]
    return llm.invoke(messages).content, docs

# --- Ø§Ø¯Ø§Ù…Ù‡ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ ---

with st.expander("ğŸ”— Ù…Ù†Ø§Ø¨Ø¹", expanded=False):
    input_urls = st.text_area("Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§:", height=100, value="https://fa.wikipedia.org/wiki/Ø¢Ø´Ù¾Ø²ÛŒ_Ø§ÛŒØ±Ø§Ù†ÛŒ")
    if st.button("ğŸ³ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ", use_container_width=True):
        if input_urls.strip():
            with st.spinner('â³ Ù¾Ø±Ø¯Ø§Ø²Ø´...'):
                # ØªÙˆÚ©Ù† Ø±Ø§ Ø¨Ù‡ ØªØ§Ø¨Ø¹ Ù¾Ø§Ø³ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
                s, r = create_knowledge_base([u.strip() for u in input_urls.split('\n') if u.strip()], hf_token)
            if s: 
                st.success(f"âœ… {r} Ø¨Ø®Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
                st.session_state["db_ready"] = True
            else: st.error(f"âŒ {r}")

if st.session_state.get("db_ready"):
    st.markdown("<br>", unsafe_allow_html=True)
    query = st.text_input("Ø³ÙˆØ§Ù„:", placeholder="Ù…Ø«Ù„Ø§Ù‹: Ú©Ø¨Ø§Ø¨...")
    if st.button("ğŸ” Ø¬Ø³ØªØ¬Ùˆ", use_container_width=True):
        if query:
            with st.spinner('ğŸ¤– ...'):
                try:
                    # ØªÙˆÚ©Ù† Ø±Ø§ Ø¨Ù‡ ØªØ§Ø¨Ø¹ Ù¾Ø§Ø³ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
                    res, docs = perform_rag_search(query, hf_token)
                    st.markdown(f'<div class="card"><div class="result-text">{res}</div></div>', unsafe_allow_html=True)
                except Exception as e: st.error(f"Ø®Ø·Ø§: {e}")

