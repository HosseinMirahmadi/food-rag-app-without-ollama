import streamlit as st
import pandas as pd
import os
import shutil

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø¨Ú©â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ø±ÙˆÛŒ Ù…ÙˆØ¨Ø§ÛŒÙ„
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
# ØªØºÛŒÛŒØ± Ù…Ù‡Ù…: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² API Ø¨Ù‡ Ø¬Ø§ÛŒ Ù…Ø¯Ù„ Ù„ÙˆÚ©Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù Ø±Ù… Ú¯ÙˆØ´ÛŒ
from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace
from langchain_community.vectorstores import Chroma

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÙˆØ¨Ø§ÛŒÙ„ ---
# ØªØºÛŒÛŒØ± layout Ø¨Ù‡ centered Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¨Ù‡ØªØ± Ø¯Ø± Ú¯ÙˆØ´ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÙˆØ¯ÛŒ
st.set_page_config(page_title="ØºØ°Ø§ Ùˆ Ø±Ø³ØªÙˆØ±Ø§Ù†", page_icon="ğŸ¥—", layout="centered")

# --- Ø§Ø³ØªØ§ÛŒÙ„â€ŒØ¯Ù‡ÛŒ Ø±ÛŒØ³Ù¾Ø§Ù†Ø³ÛŒÙˆ (Ù…Ø®ØµÙˆØµ Ù…ÙˆØ¨Ø§ÛŒÙ„) ---
st.markdown("""
<style>
    @import url('https://v1.fontapi.ir/css/Vazir');
    
    html, body, [class*="css"] { 
        font-family: 'Vazir', 'Tahoma', sans-serif; 
        direction: rtl; 
        text-align: right; 
    }
    
    .stApp { background: linear-gradient(135deg, #0f2027, #203a43, #2c5364); }
    
    /* ØªÙ†Ø¸ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¨Ø§ÛŒÙ„ */
    .stTextInput > div > div > input, .stTextArea > div > div > textarea { 
        direction: rtl; 
        text-align: right; 
        font-size: 16px; /* ÙÙˆÙ†Øª Ø¨Ø²Ø±Ú¯ØªØ± Ø¨Ø±Ø§ÛŒ ØªØ§ÛŒÙ¾ Ø±Ø§Ø­Øªâ€ŒØªØ± Ø¯Ø± Ú¯ÙˆØ´ÛŒ */
    }
    
    .card { 
        background-color: #1e1e1e; 
        padding: 15px; /* Ú©Ø§Ù‡Ø´ Ù¾Ø¯ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ ÙØ¶Ø§ÛŒ Ú©Ù… Ù…ÙˆØ¨Ø§ÛŒÙ„ */
        border-radius: 12px; 
        box-shadow: 0 4px 10px rgba(0,0,0,0.4); 
        margin-bottom: 15px; 
        border: 1px solid #333; 
    }
    
    /* Ø§Ø³ØªØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ú©Ù†Ø´â€ŒÚ¯Ø±Ø§ (Responsive) */
    @media only screen and (max-width: 600px) {
        .title { font-size: 1.8em !important; }
        .subtitle { font-size: 0.9em !important; }
        .result-text { font-size: 1em !important; line-height: 1.6 !important; }
        div[data-testid="column"] { width: 100% !important; flex: 0 0 100% !important; min-width: 100% !important; }
    }
    
    .title { font-size: 2.2em; font-weight: 800; color: #6ee7b7; text-align: right; }
    .subtitle { color: #a7f3d0; font-size: 1.1em; text-align: right; margin-top: 5px; }
    .result-text { color: #e2e8f0; font-size: 1.1em; line-height: 1.8; text-align: right; direction: rtl; }
    
    /* ØªÙ†Ø¸ÛŒÙ… Ø¬Ø¯ÙˆÙ„ Ø¯Ø± Ù…ÙˆØ¨Ø§ÛŒÙ„ */
    [data-testid="stDataFrame"] { direction: rtl; text-align: right; width: 100%; }
    .stDataFrame div[role="columnheader"], .stDataFrame div[role="gridcell"] { text-align: right !important; }
    .stAlert { direction: rtl; text-align: right; }
</style>
""", unsafe_allow_html=True)

PERSIST_DIRECTORY = "./chroma_db_food_mobile"

# --- ØªØºÛŒÛŒØ± Ø­ÛŒØ§ØªÛŒ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¨Ø§ÛŒÙ„ ---
# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² API Ø¨Ù‡ Ø¬Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø³Ù†Ú¯ÛŒÙ† Ø±ÙˆÛŒ Ú¯ÙˆØ´ÛŒ
# Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ ØªÙˆÚ©Ù† Ø±Ø§ Ø¯Ø± st.secrets Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯
def get_hf_token():
    # Ú†Ú© Ú©Ø±Ø¯Ù† ØªÙˆÚ©Ù† Ø§Ø² Ø³Ú©Ø±Øª ÛŒØ§ Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ
    if "HUGGINGFACEHUB_API_TOKEN" in st.secrets:
        return st.secrets["HUGGINGFACEHUB_API_TOKEN"]
    elif "HUGGINGFACEHUB_API_TOKEN" in os.environ:
        return os.environ["HUGGINGFACEHUB_API_TOKEN"]
    else:
        st.error("âš ï¸ ØªÙˆÚ©Ù† HuggingFace ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¢Ù† Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯.")
        return None

@st.cache_resource
def load_embedding_model():
    token = get_hf_token()
    if token:
        # Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø³Ø±ÙˆØ± Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø±Ù… Ú¯ÙˆØ´ÛŒ Ø±Ø§ Ø§Ø´ØºØ§Ù„ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        return HuggingFaceInferenceAPIEmbeddings(
            api_key=token,
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
    return None

def create_knowledge_base(urls):
    if os.path.exists(PERSIST_DIRECTORY):
        try:
            shutil.rmtree(PERSIST_DIRECTORY)
        except:
            pass
    try:
        loader = WebBaseLoader(urls)
        data = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        
        all_splits = text_splitter.split_documents(data)
        embedding_model = load_embedding_model()
        
        if embedding_model:
            vector_db = Chroma.from_documents(
                documents=all_splits,
                embedding=embedding_model,
                persist_directory=PERSIST_DIRECTORY
            )
            return True, len(all_splits)
        return False, "Ù…Ø´Ú©Ù„ Ø¯Ø± Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯"
    except Exception as e:
        return False, str(e)

def perform_rag_search(query):
    embedding_model = load_embedding_model()
    if not embedding_model:
        return "Ø®Ø·Ø§: ØªÙˆÚ©Ù† ÛŒØ§ÙØª Ù†Ø´Ø¯", []
        
    vector_db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding_model)
    retriever = vector_db.as_retriever(search_kwargs={"k": 3}) # Ú©Ø§Ù‡Ø´ k Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ù…ÙˆØ¨Ø§ÛŒÙ„
    docs = retriever.invoke(query)
    
    context_text = "\n\n".join([doc.page_content for doc in docs])
    
    token = get_hf_token()
    base_llm = HuggingFaceEndpoint(
        repo_id="HuggingFaceH4/zephyr-7b-beta",
        huggingfacehub_api_token=token,
        max_new_tokens=512,
        temperature=0.7,
        repetition_penalty=1.2
    )
    
    llm = ChatHuggingFace(llm=base_llm)
    
    messages = [
        {"role": "system", "content": "ØªÙˆ Ø¯Ø³ØªÛŒØ§Ø± Ø¢Ø´Ù¾Ø²ÛŒ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒ. Ú©ÙˆØªØ§Ù‡ Ùˆ Ø®Ù„Ø§ØµÙ‡ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡."}, # Ù¾Ø±Ø§Ù…Ù¾Øª Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¨Ø§ÛŒÙ„
        {"role": "user", "content": f"Ù…ØªÙ†:\n{context_text}\n\nØ³ÙˆØ§Ù„: {query}\n\nÙ¾Ø§Ø³Ø®:"}
    ]
    
    response = llm.invoke(messages).content
    return response, docs

# --- UI Ø¨Ø¯Ù†Ù‡ Ø§ØµÙ„ÛŒ ---
st.markdown("""
<div class="card">
    <div class="title">ğŸ¥— Ø¢Ø´Ù¾Ø²ÛŒØ§Ø± Ù‡Ù…Ø±Ø§Ù‡</div>
    <div class="subtitle">Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ØºØ°Ø§ (Ù†Ø³Ø®Ù‡ Ù…ÙˆØ¨Ø§ÛŒÙ„)</div>
</div>
""", unsafe_allow_html=True)

with st.expander("ğŸ”— Ù…Ù†Ø§Ø¨Ø¹ (Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯)", expanded=False): # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Expander Ø¨Ø±Ø§ÛŒ Ø´Ù„ÙˆØº Ù†Ø´Ø¯Ù† ØµÙØ­Ù‡ Ú¯ÙˆØ´ÛŒ
    input_urls = st.text_area(
        "Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§:",
        height=100,
        value="https://fa.wikipedia.org/wiki/Ø¢Ø´Ù¾Ø²ÛŒ_Ø§ÛŒØ±Ø§Ù†ÛŒ\nhttps://fa.wikipedia.org/wiki/Ú©Ø¨Ø§Ø¨"
    )
    
    if st.button("ğŸ³ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ù†Ø§Ø¨Ø¹", use_container_width=True): # Ø¯Ú©Ù…Ù‡ ØªÙ…Ø§Ù… Ø¹Ø±Ø¶ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¨Ø§ÛŒÙ„
        if input_urls.strip():
            url_list = [u.strip() for u in input_urls.split('\n') if u.strip()]
            with st.spinner('â³ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø¨Ø±ÛŒ...'):
                success, result = create_knowledge_base(url_list)
            if success:
                st.success(f"âœ… {result} Ø¨Ø®Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
                st.session_state["db_ready"] = True
            else:
                st.error(f"âŒ {result}")

if st.session_state.get("db_ready"):
    st.markdown("<br>", unsafe_allow_html=True)
    query = st.text_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯:", placeholder="Ù…Ø«Ù„Ø§Ù‹: Ø·Ø±Ø² ØªÙ‡ÛŒÙ‡ Ú©Ø¨Ø§Ø¨...")
    
    if st.button("ğŸ” Ø¬Ø³ØªØ¬Ùˆ", use_container_width=True): # Ø¯Ú©Ù…Ù‡ Ø¨Ø²Ø±Ú¯ Ø¨Ø±Ø§ÛŒ Ù„Ù…Ø³ Ø±Ø§Ø­Øªâ€ŒØªØ±
        if query:
            with st.spinner('ğŸ¤– ØªÙÚ©Ø±...'):
                try:
                    ai_response, source_docs = perform_rag_search(query)
                    
                    st.markdown(f"""
                    <div class="card">
                        <h3 style="color:#fbbf24; margin-bottom:10px;">ğŸ’¡ Ù¾Ø§Ø³Ø®:</h3>
                        <div class="result-text">{ai_response}</div>
                    </div>
                    """, unsafe_allow_html=True)

                    with st.expander("ğŸ“œ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…Ù†Ø§Ø¨Ø¹"):
                        table_data = []
                        for idx, doc in enumerate(source_docs):
                            table_data.append({
                                "Ù…Ù†Ø¨Ø¹": doc.metadata.get('source', 'Ù„ÛŒÙ†Ú©'),
                                "Ù…ØªÙ†": doc.page_content[:100] + "...",
                            })
                        st.table(pd.DataFrame(table_data))
            
                except Exception as e:
                    st.error(f"Ø®Ø·Ø§: {e}")

